# 1. AI? 다 똑같은 거 아니야?

## intro. 생성 AI 서비스의 종류

### Dall E

> 아마 가장 유명한 모델일 것이다.
> [DALL·E 2](https://openai.com/product/dall-e-2)
> 
- **Transformer** 언어 모델 + **GAN(Generative Adversarial Network)** 이미지 생성 모델
- 먼저 입력된 문장을 인코딩하여 의미를 추출하고, 그 다음 이를 디코딩하여 이미지를 생성함

### 미드 저니

> **전세계 1500만 구독자… 미술 대회 입상 경력 보유의 컬쳐 쇼크**
> [Midjourney](https://www.midjourney.com/)
> 
- Discord 기반 구독 형 생성 AI 서비스이다.
- 내부 로직이나 모델에 관련하여 공개된 부분은 없다.
- 오타쿠를 위한 니지 저니가 존재한다.
    
    [niji・journey](https://nijijourney.com/ko/)
    

### Stable Diffusion

> 오픈소스로 공개된 뮌헨 대학의 최고의 아웃 풋. 생성 AI의 희망이자 혁신 오픈소스
> [GitHub - Stability-AI/stablediffusion](https://github.com/Stability-AI/stablediffusion)
> 
- Diffusion 모델을 기반으로 이미지를 생성 하는 생성 AI 이다.
- 2023.04 부 2.1 버전까지 공개되었고, 3.0에 들어갈 기술들이 개발되고 있다.
- Stable Diffusion Webui (by Automatic 1111) 을 기반으로 일반인들도 쉽게 쓸 수 있게 되었다.
    
    [GitHub - AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui)
    

# 1. 그래픽 AI의 기술적 차이

> 근본 중의 근본인 CNN부터 RNN, GAN, Diffusion 순으로 설명하고자 한다.
> 

## 1-1. CNN

- 편하게 이해하기 CNN편
    1. CNN은 쉽게 말해서, 특징을 추출하는 방법론이다.
        <img src=".\asset\CNN_1.png" width="500" >
        
    2. 이미지를 배열 데이터로 전환하고,
        
        <img src=".\asset\CNN_2.png" width="500" >
        
        - 이 때, 흑백 이미지의 데이터는 2차원 배열, 컬러 이미지는 3차원 배열 (depth=3) 이다.
            - [TIP] 직접 CNN 구현 시, PNG 파일의 경우 RBG가 아니라 RBGA로 들어온다.
                - Transparent한 부분(보통은 배경)을 Black으로 자동 전환하기 때문에,
                White로 바꿔주는 로직을 추가하지 않으면 모델이 이미지 인식할 때, 뻑난다.
    3. 이렇게 배열 데이터에서 특정 패턴을 뽑아내기 위한 필터를 적용하여 특징들을 추출하고
        
        <img src=".\asset\CNN_3.png" width="500" >
        
        - 이 때, 필터 값 배치 및 가중치는 랜덤이다.
    4. 특징을 추출한 배열 데이터를 Pooling으로 작게 만드는 과정을 반복한다.
    
- RNN과의 차이
    - RNN은 이어진 데이터에 대해 인식하는 데 특화되어 있다.
        - 즉, 앞 뒤의 관계성이 중요한 텍스트 데이터 및 음성 데이터 분석에 강점을 가진다.
    - CNN은 2차원 배열에서의 패턴인식에 특화되어있다.
        - 즉, 이미지 데이터 혹은 음성 데이터(음성 패턴을 이미지화 하여 분석)에 강점을 가진다.
        - 음성 인식 부분에서, 음성 그래프를 이미지 처럼 읽어 오는 방식이 상당히 참신했다.
            - 관련하여 배경 소음 속에서 아기의 울음소리를 인식하는 프로젝트가 있었다.

## 1-2. GAN

- 편하게 이해하기 GAN편
    
    쉽게 말해서, GAN은 두 놈을 싸움 붙여서 서로 성장시키는 모델이다.
    
    [GAN의 개념과 이해 | 인사이트리포트 | 삼성SDS](https://www.samsungsds.com/kr/insights/generative-adversarial-network-ai-2.html)
    
    [GAN 소개 — PseudoLab Tutorial Book](https://pseudo-lab.github.io/Tutorial-Book/chapters/GAN/Ch1-Introduction.html)
    
    1. 생성 모델은 가짜 데이터를 최대한 진짜처럼 보이게 만들고,
    2. 분류 모델은 가짜 데이터를 최대한 잘 가려내도록 한다.
    
    이러한 과정이 계속 반복되면서 성장하는 것이 GAN 모델이다.
    
- 구조를 도식화 하면 아래와 같다.
    
    <img src=".\asset\CNN_4.png" width="500" >
    
- 사용하는 함수는 아래와 같다.
    
    <img src=".\asset\CNN_5.png" width="500" >
    
    - 구분기(D):
    실제 데이터(x)를 입력하면 D(x)가 커지면서 log값이 커지면서 높은 확률값이 나오도록 하고, 가짜 데이터(G(z))를 입력하면 log값이 작아짐에 따라 낮은 확률값이 나오도록 학습
    - 생성기(G):
    Zero-Mean Gaussian 분포에서 노이즈 z를 멀티레이어 퍼셉트론에 통과시켜 샘플들을 생성하며 이 생성된 가짜 데이터 G(z)를 D에 input으로 넣었을 때 실제 데이터처럼 확률이 높게 나오도록 학습
    - 즉, 통짜로 생성한 이미지를 기반으로 분류기에 넣고 분류합니다.
- 결론적으로, 이미지가 진짜(1)인지 가짜(0)인지 구분하지 못하는 경계점(0.5)으로 가는 것이 목표.

## 1-3. Tansformer

- 편하게 이해하기 Dall E 편
    
    쉽게 말해서, 텍스트 기반으로 각 가중치에 맞게 적절한 이미지를 생성해줍니다.
    
- Dall E의 기반 매커니즘.
    - DALL E는 GPT를 기반으로 만든 모델입니다.
    즉, 다른 생성 AI와는 다르게 그 근본이 Text 모델이기 때문에 가지는 특성이 있습니다.
    - Dall E의 근본이 되는 Transformer는 기존의 RNN과 CNN 기반 모델과 달리, 
    **Attention 메커니즘**을 사용하여 입력 시퀀스의 모든 단어를 한 번에 처리할 수 있습니다. 
    이를 통해, 입력 시퀀스의 길이에 상관없이 일정한 수준의 성능을 보입니다.
        - Attention 매커니즘이란?
            
            Attention 메커니즘은 다음과 같은 방법으로 동작합니다.
            
            1. 쿼리(Query) 벡터와 키(Key) 벡터, 값(Value) 벡터를 생성합니다. 이때, 쿼리 벡터는 출력 시퀀스의 현재 단어를 나타내며, 키 벡터와 값 벡터는 입력 시퀀스의 각 단어를 나타냅니다.
            2. 각 단어의 중요도를 나타내는 가중치를 계산합니다. 이때, 쿼리 벡터와 각 단어의 키 벡터의 내적을 계산하여 중요도를 나타내는 스칼라 값을 생성합니다.
            3. 가중치와 값 벡터를 사용하여 출력 벡터를 계산합니다. 이때, 가중치와 값 벡터의 가중 평균을 계산하여 출력 벡터를 생성합니다.
    - 즉, 프롬프트에 대한 가중치가 자동으로 조정되는 특성을 가지기에 SD와 다르게 프롬프트에 대한 접근성이 높습니다.
        - 단, 다르게 말하면 프롬프트에 대한 조정치를 커스텀할 수 없습니다.

## 1-4. Stable Diffusion

- 편하게 이해하기. SD편
    
    쉽게 말해서, Ai 모델을 가스라이팅 하는 것이다.
    아무것도 없는 곳에서 “이건 ~~를 극한까지 노이즈화 시킨거다.” 라고 말하고
    그 아무것도 없는 노이즈부터 원본 이미지를 생성 시키는 방법이다.
    
    <img src=".\asset\CNN_6.png" width="500" >
    
- LDM (Latent Diffusion Model):
    
    SD의 심장 같은 친구다. 원래도 효율적이던 Diffusion을 날아오르게 만든 기법
    
    <img src=".\asset\CNN_7.png" width="500" >
    
    [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
    
    - LDM의 장점(중요하지는 않음): 쉽게 말해서, 가성비가 좋다.
        1. 효율적인 샘플링: 
        잠재 공간을 사용함으로써 더 효율적인 샘플링이 가능해집니다. 잠재 공간에서의 샘플링은 원본 이미지 공간에서의 샘플링보다 계산 비용이 적게 들고, 더 나은 확률 분포 추정이 가능합니다.
        2. 더 나은 이미지 품질: 
        잠재 공간에서의 확률 분포를 사용하여 샘플링하면, 생성된 이미지의 품질이 높아질 확률이 높습니다. 이는 학습된 확률 분포가 실제 이미지 분포와 가까워지기 때문입니다.
        3. 구조적인 이미지 생성: 
        LDM은 이미지의 구조와 특성을 잘 보존하면서 이미지를 생성할 수 있습니다. 잠재 공간에서의 샘플링과 변환은 이미지의 구조를 유지하면서 새로운 이미지를 생성하는 데 도움이 됩니다.
        4. 조건부 생성: 
        LDM은 조건부 생성을 지원합니다. 즉, 특정한 조건을 만족하는 이미지를 생성하는 것이 가능합니다. 예를 들어, 특정 클래스의 이미지나 특정 스타일의 이미지를 생성할 수 있습니다.
        5. 이미지 간의 보간: 
        잠재 공간에서의 거리가 이미지 간의 유사성을 나타내므로, 이미지 간의 보간이 가능합니다. 즉, 두 이미지 사이의 연속적인 변화를 나타내는 이미지 시퀀스를 생성할 수 있습니다.
- SD의 작동 방식
    
    <img src=".\asset\CNN_8.png" width="500" >
    
    1. Latent Space(잠재 공간): 
    이미지의 특성을 표현하는 저차원 공간입니다. 
    잠재 공간에서의 점은 원본 이미지 공간에서의 이미지와 대응됩니다. 
    이 공간에서의 거리는 이미지 간의 유사성을 나타내며, 이미지 생성 과정은 이 잠재 공간에서의 샘플링과 변환을 통해 이루어집니다.
    → 쉽게 말해서 처음 시작이 점 한 개(.) 라면, 목표로 하는 그림을 위해서는 어떻게 그림을 그려나가야 하는지 나열되는 샘플링들과 비교해가는 장소
    2. Diffusion Process(디퓨전 과정):
    노이즈가 있는 이미지를 점차 더 깨끗한 이미지로 바꾸는 과정입니다. 
    LDM은 이 디퓨전 과정을 역으로 진행하여 이미지를 생성합니다. 
    이 과정에서 잠재 공간의 샘플링이 사용되며, 잠재 공간에서의 확률 분포를 따라 이미지가 생성됩니다.
    → 디 노이징 과정은, 샘플링 이미지 중에서 가장 합리적인 친구를 정하는 과정
    3. Neural Networks: 
    LDM은 딥러닝 모델을 사용하여 디퓨전 과정을 역으로 진행하고, 잠재 공간에서의 샘플링을 수행합니다. 
    이러한 딥러닝 모델은 이미지의 특성을 추출하고, 잠재 공간과 원본 이미지 공간 사이의 변환을 학습합니다. 또한, 생성 과정에서의 샘플링과 이미지 변환을 지원합니다.